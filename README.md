This Wiki crawler is based on **Scrapy** and plots outputs using **Matplotlib**. The file, items.json, is generated after 500 wiki pages are crawled and includes 3 types of outcomes: success, which indicates that there is a path to Philosophy page; cycle, which indicates that there is a loop that redirects to the previously visited page; and dead end, which indicates that there are no more links that can be scraped. Please check the result.txt for how many pages, the percentage of pages and average path length of pages that have successfully gotten to Philosophy. Result.png is the final graph of the distribution of the frequency of path lengths directed to Philosophy for 500 random wiki pages analyzed.

After analysing the results, I found that most of the pages loop in a cycle. Most of the pages go to the fact page, then go to reality page, then go to exist page, then loop back to reality pagain (fact->reality->exist->reality). According to this wiki page: [Getting_to_Philosophy](https://en.wikipedia.org/wiki/Wikipedia_talk:Getting_to_Philosophy), previously, for every random wiki page, a high percentage used to redirect to the Philosophy page, but not any more.

Simply type python main.py to run the crawler after installing the dependencies in the requirements.txt, which after finishing, will generate the items.json, result.txt and result.png.

Things that can be improved:
* Crawl Wikipedia mobile links. Usually mobile pages can contain over 90% less data than full-sized pages while still retaining the necessary information and it's easier to crawl based on my past experience.
* Maintain a hashmap of visited links and their distances, which signifcantly reduces the amount of redundant link traversals and requests on finding already visited pages. It's like a cache layer of the crawler, the problem with in memory hashmap is it'll lose the information after the crawling process is finished. We can use database or even Redis to serve as a cache layer, in that way we'll always store the visited pages to prevent requesting duplicate pages. 
* The reason I chose Scrapy to implement the crawler is not only because it's production ready code, but Scrapy makes request asynchronously. It takes a longer time to scrape and get links then to request pages. For that reason making requests asynchronously can significantly improve the performance of the crawler. Additionally the performance can be improved by running the crawler multiprocessingly or even try scaling our web crawler to different machines.
* Log and monitor system. We can provide RESTful api for the metric health information of the crawler, or even api to start and stop. After that we can integrate with front end to provide a user-friendly page for easy operation.
